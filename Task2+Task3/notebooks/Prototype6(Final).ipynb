{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "pLJgIN7_ccyY",
        "outputId": "1292dbdc-fbc3-463a-94da-75726c024cc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Collecting dateparser\n",
            "  Downloading dateparser-1.2.2-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser) (2025.11.3)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser) (5.3.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7.0->dateparser) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Downloading dateparser-1.2.2-py3-none-any.whl (315 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.5/315.5 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dateparser\n",
            "Successfully installed dateparser-1.2.2\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting newspaper3k\n",
            "  Downloading newspaper3k-0.2.8-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (4.13.5)\n",
            "Requirement already satisfied: Pillow>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (11.3.0)\n",
            "Requirement already satisfied: PyYAML>=3.11 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.3)\n",
            "Collecting cssselect>=0.9.2 (from newspaper3k)\n",
            "  Downloading cssselect-1.4.0-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: lxml>=3.6.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (6.0.2)\n",
            "Requirement already satisfied: nltk>=3.2.1 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (3.9.1)\n",
            "Requirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.32.4)\n",
            "Collecting feedparser>=5.2.1 (from newspaper3k)\n",
            "  Downloading feedparser-6.0.12-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting tldextract>=2.0.1 (from newspaper3k)\n",
            "  Downloading tldextract-5.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting feedfinder2>=0.0.4 (from newspaper3k)\n",
            "  Downloading feedfinder2-0.0.4.tar.gz (3.3 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jieba3k>=0.35.1 (from newspaper3k)\n",
            "  Downloading jieba3k-0.35.1.zip (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from newspaper3k) (2.9.0.post0)\n",
            "Collecting tinysegmenter==0.3 (from newspaper3k)\n",
            "  Downloading tinysegmenter-0.3.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->newspaper3k) (4.15.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from feedfinder2>=0.0.4->newspaper3k) (1.17.0)\n",
            "Collecting sgmllib3k (from feedparser>=5.2.1->newspaper3k)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.2.1->newspaper3k) (4.67.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.10.0->newspaper3k) (2026.1.4)\n",
            "Collecting requests-file>=1.4 (from tldextract>=2.0.1->newspaper3k)\n",
            "  Downloading requests_file-3.0.1-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.12/dist-packages (from tldextract>=2.0.1->newspaper3k) (3.20.3)\n",
            "Downloading newspaper3k-0.2.8-py3-none-any.whl (211 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.1/211.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cssselect-1.4.0-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.12-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.5/81.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tldextract-5.3.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.9/105.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-3.0.1-py2.py3-none-any.whl (4.5 kB)\n",
            "Building wheels for collected packages: tinysegmenter, feedfinder2, jieba3k, sgmllib3k\n",
            "  Building wheel for tinysegmenter (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tinysegmenter: filename=tinysegmenter-0.3-py3-none-any.whl size=13540 sha256=49ba0a92e92d60897e8408d667fc184814c54d91b808987bf1601f9d00ce2910\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/91/9f/00d66475960891a64867914273fcaf78df6cb04d905b104a2a\n",
            "  Building wheel for feedfinder2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for feedfinder2: filename=feedfinder2-0.0.4-py3-none-any.whl size=3341 sha256=6a666c0c741de7a0ded326b7a53d027670a725c69799a2a516e67bc0506bced4\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/9f/fb/364871d7426d3cdd4d293dcf7e53d97f160c508b2ccf00cc79\n",
            "  Building wheel for jieba3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jieba3k: filename=jieba3k-0.35.1-py3-none-any.whl size=7398380 sha256=259ea849da143b5f238c9d55ab5970ff98ef4744a56ca3adb31a9db6d365d6ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/26/72/f7/fff392a8d4ea988dea4ccf9788599d09462a7f5e51e04f8a92\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=fa0f36ad1c9be548452b53c5108c710a0bae50372af97828e7d946c90589cbaf\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built tinysegmenter feedfinder2 jieba3k sgmllib3k\n",
            "Installing collected packages: tinysegmenter, sgmllib3k, jieba3k, feedparser, cssselect, requests-file, feedfinder2, tldextract, newspaper3k\n",
            "Successfully installed cssselect-1.4.0 feedfinder2-0.0.4 feedparser-6.0.12 jieba3k-0.35.1 newspaper3k-0.2.8 requests-file-3.0.1 sgmllib3k-1.0.0 tinysegmenter-0.3 tldextract-5.3.1\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from lxml_html_clean) (6.0.2)\n",
            "Downloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: lxml_html_clean\n",
            "Successfully installed lxml_html_clean-0.4.3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-text-splitters\n",
            "  Downloading langchain_text_splitters-1.1.0-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: langchain-core<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-text-splitters) (1.2.7)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.6.4)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.13.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.4.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (4.12.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2026.1.4)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core<2.0.0,>=1.2.0->langchain-text-splitters) (2.5.0)\n",
            "Downloading langchain_text_splitters-1.1.0-py3-none-any.whl (34 kB)\n",
            "Installing collected packages: langchain-text-splitters\n",
            "Successfully installed langchain-text-splitters-1.1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.8/23.8 MB\u001b[0m \u001b[31m31.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m58.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.0/51.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting trafilatura\n",
            "  Downloading trafilatura-2.0.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2026.1.4)\n",
            "Requirement already satisfied: charset_normalizer>=3.4.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (3.4.4)\n",
            "Collecting courlan>=1.3.2 (from trafilatura)\n",
            "  Downloading courlan-1.3.2-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting htmldate>=1.9.2 (from trafilatura)\n",
            "  Downloading htmldate-1.9.4-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting justext>=3.0.1 (from trafilatura)\n",
            "  Downloading justext-3.0.2-py2.py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (6.0.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26 in /usr/local/lib/python3.12/dist-packages (from trafilatura) (2.5.0)\n",
            "Requirement already satisfied: babel>=2.16.0 in /usr/local/lib/python3.12/dist-packages (from courlan>=1.3.2->trafilatura) (2.17.0)\n",
            "Collecting tld>=0.13 (from courlan>=1.3.2->trafilatura)\n",
            "  Downloading tld-0.13.1-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: dateparser>=1.1.2 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (1.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.9.0.post0 in /usr/local/lib/python3.12/dist-packages (from htmldate>=1.9.2->trafilatura) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2024.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.2)\n",
            "Requirement already satisfied: regex>=2024.9.11 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (2025.11.3)\n",
            "Requirement already satisfied: tzlocal>=0.2 in /usr/local/lib/python3.12/dist-packages (from dateparser>=1.1.2->htmldate>=1.9.2->trafilatura) (5.3.1)\n",
            "Requirement already satisfied: lxml_html_clean in /usr/local/lib/python3.12/dist-packages (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura) (0.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.9.0.post0->htmldate>=1.9.2->trafilatura) (1.17.0)\n",
            "Downloading trafilatura-2.0.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.6/132.6 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading courlan-1.3.2-py3-none-any.whl (33 kB)\n",
            "Downloading htmldate-1.9.4-py3-none-any.whl (31 kB)\n",
            "Downloading justext-3.0.2-py2.py3-none-any.whl (837 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.9/837.9 kB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tld-0.13.1-py2.py3-none-any.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tld, courlan, justext, htmldate, trafilatura\n",
            "Successfully installed courlan-1.3.2 htmldate-1.9.4 justext-3.0.2 tld-0.13.1 trafilatura-2.0.0\n"
          ]
        }
      ],
      "source": [
        "#Installing all the necessary libraries and importing them(after running it will tell you to restart runtime but no need to do that, it works)\n",
        "!pip install spacy dateparser\n",
        "!python -m spacy download en_core_web_sm\n",
        "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import re\n",
        "import spacy\n",
        "import dateparser\n",
        "import time\n",
        "import requests\n",
        "import json\n",
        "import numpy as np\n",
        "!pip install newspaper3k\n",
        "!pip install lxml_html_clean\n",
        "import nltk\n",
        "nltk.download('punkt') # Essential for parsing and NLP tasks\n",
        "from newspaper import Article\n",
        "!pip install langchain-text-splitters\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "!pip install -qU langchain-huggingface\n",
        "!pip install -qU langchain-community faiss-cpu\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "!pip install trafilatura\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import timedelta"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive', force_remount = True)\n",
        "# %cd drive/MyDrive/projects/GDG/Task2\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1fK6f70yMpSOY9tnsbQ6yS3GL2TrlrpBf' -O ticker_dataset.json #downloading dataset of tickers"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QeHK_3etwyAf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "554e1d09-9cf2-4c78-abd0-a4af4a9b9ec5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-29 07:31:53--  https://docs.google.com/uc?export=download&id=1fK6f70yMpSOY9tnsbQ6yS3GL2TrlrpBf\n",
            "Resolving docs.google.com (docs.google.com)... 142.251.31.113, 142.251.31.101, 142.251.31.102, ...\n",
            "Connecting to docs.google.com (docs.google.com)|142.251.31.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://drive.usercontent.google.com/download?id=1fK6f70yMpSOY9tnsbQ6yS3GL2TrlrpBf&export=download [following]\n",
            "--2026-01-29 07:31:53--  https://drive.usercontent.google.com/download?id=1fK6f70yMpSOY9tnsbQ6yS3GL2TrlrpBf&export=download\n",
            "Resolving drive.usercontent.google.com (drive.usercontent.google.com)... 173.194.79.132, 2a00:1450:4013:c05::84\n",
            "Connecting to drive.usercontent.google.com (drive.usercontent.google.com)|173.194.79.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 410124 (401K) [application/octet-stream]\n",
            "Saving to: ‘ticker_dataset.json’\n",
            "\n",
            "ticker_dataset.json 100%[===================>] 400.51K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2026-01-29 07:31:55 (25.4 MB/s) - ‘ticker_dataset.json’ saved [410124/410124]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "D-AhckTWc_2c"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#here I am using regex and nlp to parse the user prompt to detect organizations, time, tickers and convert the detected dates into datetime objects\n",
        "TICKER_RE = re.compile(r\"\\$?([A-Z]{1,5})(?:\\b|$)\") #regex\n",
        "\n",
        "def parse_question(q):\n",
        "  doc = nlp(q) #nlp\n",
        "  orgs = []\n",
        "  dates = []\n",
        "  tickers = []\n",
        "  for m in TICKER_RE.finditer(q):\n",
        "    tickers.append(m.group(1))\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ in (\"ORG\",):\n",
        "      orgs.append(ent.text)\n",
        "    if ent.label_ in (\"PRODUCT\",):\n",
        "      orgs.append(ent.text)\n",
        "    if ent.label_ in (\"DATE\", \"TIME\", \"ORDINAL\"):\n",
        "      dates.append(ent.text)\n",
        "  date_ranges = []\n",
        "  for d in dates:\n",
        "    parsed = dateparser.parse(d)\n",
        "    if parsed:\n",
        "      date_ranges.append(parsed.date())\n",
        "  quarter_match = re.search(r\"(Q[1-4]\\s*\\d{4}|quarter\\s*(1|2|3|4)\\s*(\\d{4})?)\", q, re.I)\n",
        "  if quarter_match:\n",
        "        qstr = quarter_match.group(0)\n",
        "        date_ranges.append(qstr)\n",
        "  print(f\"Organizations detected : {orgs}\")\n",
        "  # print(type(orgs[0]))\n",
        "  print(f\"Dates deetcted : {dates}\")\n",
        "  print(f\"Tickers detected: {tickers}\")\n",
        "  print(f\"parsed dates : {date_ranges}\")\n",
        "  return [orgs, dates, tickers, date_ranges]\n"
      ],
      "metadata": {
        "id": "rce6juDudJUU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = \"4Y2ULGZ29S1K2FAG\"\n",
        "def map_company_to_ticker(name): #this function converts detected companies from prompt into their tickers\n",
        "  p = Path(\"ticker_dataset.json\")\n",
        "  df = pd.read_json(p)\n",
        "  try:\n",
        "    matches = df[df[\"Security Name\"].str.contains(name, case=False, na=False)]['Symbol'].iloc[0] #if it finds ticker in the downloaded database then everything is fine :)\n",
        "  except:\n",
        "    params = {\"function\":\"SYMBOL_SEARCH\",\"keywords\":name,\"apikey\":API_KEY}\n",
        "    r = requests.get(\"https://www.alphavantage.co/query\", params=params)\n",
        "    js = r.json()\n",
        "    matches = [\n",
        "      {\"symbol\": m[\"1. symbol\"], \"name\": m[\"2. name\"]}\n",
        "      for m in js.get(\"bestMatches\", [])\n",
        "    ]\n",
        "    try:\n",
        "      matches = matches[0]['symbol'] #otherwise I try to use alphavantage to covert company to ticker which is rarely accurate but last resort\n",
        "    except:\n",
        "      print(\"No matches found\")\n",
        "  return matches\n",
        "  # for i in range(2):\n",
        "  #   print(matches[i]['symbol'])"
      ],
      "metadata": {
        "id": "sunya-80CeuJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def add_tickers(parsed_question): #update the parsed_question list\n",
        "  for company in parsed_question[0]:\n",
        "    print(company)\n",
        "    time.sleep(1.5)\n",
        "    matches = map_company_to_ticker(company)\n",
        "    if matches and matches not in parsed_question[2]:\n",
        "      print(f\"Found match {matches}\")\n",
        "      parsed_question[2].append(matches)\n",
        "    else:\n",
        "      print(f\"No ticker found for company: {company}\")"
      ],
      "metadata": {
        "id": "0WHP9bsQDf70"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BASE = \"https://www.alphavantage.co/query\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "x1ZNI6ixO9Oc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_news_for_ticker(ticker,parsed_question,page=2): #This function is fetching time specific news using alpha vantage\n",
        "  if not parsed_question[3]: #no time specified\n",
        "    params = {\n",
        "        \"function\" : \"NEWS_SENTIMENT\",\n",
        "        \"tickers\" : ticker,\n",
        "        \"sort\": \"RELEVANCE\",\n",
        "        \"limit\": \"50\",\n",
        "        \"apikey\": API_KEY\n",
        "    }\n",
        "  elif len(parsed_question[3])==1 : #one datetime object\n",
        "    # parsed_question[3].sort()\n",
        "    time_from = str(parsed_question[3][0].year).zfill(2) + str(parsed_question[3][0].month).zfill(2) + str(parsed_question[3][0].day).zfill(2) + \"T0000\"\n",
        "    next_year = parsed_question[3][0] + timedelta(days = 365)\n",
        "    time_to = str(next_year.year).zfill(2) + str(next_year.month).zfill(2) + str(next_year.day).zfill(2) + \"T0000\"\n",
        "    params = {\n",
        "        \"function\" : \"NEWS_SENTIMENT\",\n",
        "        \"tickers\" : ticker,\n",
        "        \"time_from\" : time_from,\n",
        "        \"time_to\" : time_to,\n",
        "        \"sort\": \"RELEVANCE\",\n",
        "        \"limit\": \"50\",\n",
        "        \"apikey\": API_KEY\n",
        "    }\n",
        "    print(f\"Time detected, fetching news accordingly{time_from}-{time_to}\")\n",
        "  else: #multiple datetime objects\n",
        "    parsed_question[3].sort()\n",
        "    time_from = str(parsed_question[3][0].year).zfill(2) + str(parsed_question[3][0].month).zfill(2) + str(parsed_question[3][0].day).zfill(2) + \"T0000\"\n",
        "    time_to = str(parsed_question[3][1].year).zfill(2) + str(parsed_question[3][1].month).zfill(2) + str(parsed_question[3][1].day).zfill(2) + \"T0000\"\n",
        "    params = {\n",
        "        \"function\" : \"NEWS_SENTIMENT\",\n",
        "        \"tickers\" : ticker,\n",
        "        \"time_from\" : time_from,\n",
        "        \"time_to\" : time_to,\n",
        "        \"sort\": \"RELEVANCE\",\n",
        "        \"limit\": \"50\",\n",
        "        \"apikey\": API_KEY\n",
        "    }\n",
        "    print(f\"Time detected, fetching news accordingly{time_from}-{time_to}\")\n",
        "  r = requests.get(BASE, params = params)\n",
        "  return r.json().get(\"feed\",[])\n",
        "\n",
        "def is_significant_sentiment(entry, ticker, threshold=0.15): #filtering news on basis of sentiment score\n",
        "    for t in entry.get(\"ticker_sentiment\", []):\n",
        "        if t[\"ticker\"] == ticker:\n",
        "            score = float(t[\"ticker_sentiment_score\"])\n",
        "            return abs(score) >= threshold and t[\"ticker_sentiment_label\"] != \"Neutral\"\n",
        "    return False\n",
        "\n",
        "import trafilatura\n",
        "import requests\n",
        "from requests.adapters import HTTPAdapter\n",
        "from urllib3.util.retry import Retry\n",
        "\n",
        "def get_article_text(url):\n",
        "  try:\n",
        "      # Configure requests session with no retries\n",
        "      session = requests.Session()\n",
        "      retry_strategy = Retry(total=0, backoff_factor=0.1, status_forcelist=[429, 500, 502, 503, 504])\n",
        "      adapter = HTTPAdapter(max_retries=retry_strategy)\n",
        "      session.mount(\"http://\", adapter)\n",
        "      session.mount(\"https://\", adapter)\n",
        "\n",
        "      # Fetch the URL using the configured session\n",
        "      response = session.get(url, timeout=30)\n",
        "      response.raise_for_status() # Raise an exception for bad status codes\n",
        "\n",
        "      downloaded_html = response.text\n",
        "      if not downloaded_html:\n",
        "          print(f\"Failed to download content from {url}: Empty HTML\")\n",
        "          return None\n",
        "      else:\n",
        "        print(f\"Successfully downloaded HTML for {url}\")\n",
        "\n",
        "      # Extract text using trafilatura from the downloaded HTML\n",
        "      text = trafilatura.extract(downloaded_html, url=url, include_comments=False, include_tables=False)\n",
        "      return text\n",
        "  except requests.exceptions.RequestException as e:\n",
        "      print(f\"Error processing {url} with requests: {e}\")\n",
        "      return None\n",
        "  except Exception as e:\n",
        "      print(f\"Error processing {url}: {e}\")\n",
        "      return None\n",
        "\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "def build_docs(tickers, parsed_question): #building vectordatabase with fetched new, I try to include stuff like tickers and time published so similarity search is more accurate later on\n",
        "  docs = []\n",
        "  for ticker in tickers:\n",
        "    articles = fetch_news_for_ticker(ticker,parsed_question=parsed_question, page = 2)\n",
        "    for entry in articles:\n",
        "        if is_significant_sentiment(entry, ticker):\n",
        "            full_text = get_article_text(entry[\"url\"]) or \"\"\n",
        "            content = full_text if full_text else f\"{entry.get('title')}\\n\\n{entry.get('summary')}\"\n",
        "            docs.append(Document(\n",
        "                page_content=content,\n",
        "                metadata={\n",
        "                    \"title\": entry.get(\"title\"),\n",
        "                    \"sentiment_label\": entry.get(\"ticker_sentiment\", [{}])[0].get(\"ticker_sentiment_label\"),\n",
        "                    \"sentiment_score\": entry.get(\"ticker_sentiment\", [{}])[0].get(\"ticker_sentiment_score\"),\n",
        "                    \"url\": entry.get(\"url\"),\n",
        "                    \"time_published\": entry.get(\"time_published\"),\n",
        "                    \"tickers\": [t.get(\"ticker\") for t in entry.get(\"ticker_sentiment\", [])]\n",
        "                }\n",
        "            ))\n",
        "  print(f\"Loaded {len(docs)} documents\")\n",
        "  return docs\n"
      ],
      "metadata": {
        "id": "4ppctvV6JEzq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def retrieve(question, k = 10):\n",
        "  results = db.similarity_search(question, k)   # returns Document objects\n",
        "  text = []\n",
        "  for i, r in enumerate(results, 1):\n",
        "      # print(f\"\\n=== Result {i} ===\")\n",
        "      # print(\"Metadata:\", r.metadata)\n",
        "      # print(\"Text snippet:\", r.page_content[:])\n",
        "      text.append(r.page_content[:])\n",
        "  return text\n"
      ],
      "metadata": {
        "id": "xixhht1EGeon"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import base64\n",
        "from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
        "\n",
        "# Use getpass to keep your token secure\n",
        "# if \"HUGGINGFACEHUB_API_TOKEN\" not in os.environ:\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = base64.b64decode(\"aGZfb2pXUWFiSm5wUWR1VmtUVkNPTFFIbGN5RGlBRFd5cE1sVA==\").decode()\n",
        "\n",
        "llm = HuggingFaceEndpoint(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\", #Using qwen cuz it has better reasoning than other comparable models so it can read the news and better corelate it with the fluctuations in prices\n",
        "    task = \"text-generation\",\n",
        "    temperature=0.7,\n",
        "    max_new_tokens=2048 # Moved out of model_kwargs\n",
        ")\n",
        "model = ChatHuggingFace(llm=llm)"
      ],
      "metadata": {
        "id": "iXzgy_3kHdlR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "response = model.invoke([HumanMessage(content=\"tell me about stocks of samsung\")]) #Baseline reponse without RAG\n",
        "print(response.content)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "UfpVsHVwKm7X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f86eba35-240d-4050-9afc-2df66ebb4659"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, I can provide you with an overview of Samsung's stock. Samsung is a South Korean multinational conglomerate headquartered in Samsung Town, Seoul. The company is a global leader in electronics and technology, with a wide range of products including smartphones, televisions, semiconductors, and more.\n",
            "\n",
            "### Key Points About Samsung's Stock:\n",
            "\n",
            "1. **Company Overview**:\n",
            "   - **Name**: Samsung Electronics Co., Ltd.\n",
            "   - **Industry**: Technology and Electronics\n",
            "   - **Headquarters**: Seoul, South Korea\n",
            "   - **Market Cap**: Samsung Electronics is one of the largest publicly traded companies by market capitalization.\n",
            "\n",
            "2. **Stock Information**:\n",
            "   - **Ticker Symbol**: SNE (NASDAQ)\n",
            "   - **Stock Exchange**: NASDAQ\n",
            "   - **Stock Dividend Yield**: Historically, Samsung has paid dividends, but the yield can vary. As of the latest data, Samsung's dividend yield is relatively low, typically around 1-2%.\n",
            "   - **Price-to-Earnings (P/E) Ratio**: The P/E ratio is a valuation metric that compares a company's current share price to its earnings per share. This can fluctuate based on market conditions and the company's financial performance.\n",
            "\n",
            "3. **Financial Performance**:\n",
            "   - **Revenue**: Samsung is one of the world's largest technology companies, with significant revenue from both hardware and services.\n",
            "   - **Margins**: The company is known for its strong profit margins, particularly in the semiconductor and mobile phone markets.\n",
            "   - **Growth**: Samsung has shown consistent growth in recent years, driven by increasing demand for smartphones and advancements in technology.\n",
            "\n",
            "4. **Investment Considerations**:\n",
            "   - **Volatility**: Like many tech stocks, Samsung's stock can be volatile, influenced by market conditions, semiconductor supply chain issues, and global economic factors.\n",
            "   - **Diversification**: As with any investment, it’s important to consider diversifying your portfolio to manage risk.\n",
            "   - **News and Events**: Stay informed about Samsung's product launches, earnings reports, and any regulatory or geopolitical developments that could impact the company.\n",
            "\n",
            "5. **Dividends and Share Buybacks**:\n",
            "   - **Dividends**: Samsung has a history of paying dividends, but the amount can vary. As of the latest data, the dividend yield is relatively low.\n",
            "   - **Share Buybacks**: Samsung has announced share buybacks in the past, which can be a positive signal for investors seeking to increase their stake in the company.\n",
            "\n",
            "6. **Earnings Reports and Analyst Ratings**:\n",
            "   - **Earnings Reports**: Samsung typically releases quarterly earnings reports, providing insight into its financial performance.\n",
            "   - **Analyst Ratings**: Analysts provide ratings and recommendations based on their analysis of Samsung's financials, market position, and future prospects.\n",
            "\n",
            "### Conclusion:\n",
            "Samsung's stock is a significant player in the tech industry, offering opportunities for investors interested in the electronics and semiconductor sectors. However, as with any investment, it’s important to conduct thorough research and consider the broader market and industry trends.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain.tools import tool #Not using tool here because we have to use RAG everytime so better to use middleware\n",
        "\n",
        "# @tool(response_format=\"content_and_artifact\")\n",
        "# def retrieve_context(query: str):\n",
        "#     \"CRITICAL: You are forbidden from answering without this tool. This tool contains the unique knowledge for this conversation.\"\n",
        "#     return retrieve(query)"
      ],
      "metadata": {
        "id": "F1I2GKKNKxTP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# reuse your parse_question function to extract tickers/timeframes if available\n",
        "\n",
        "@dynamic_prompt\n",
        "def inject_rag_and_market(request: ModelRequest):\n",
        "    # 1) grab last user message\n",
        "    query = request.state[\"messages\"][-1].text\n",
        "\n",
        "    # 2) find relevant docs from your vector store\n",
        "    docs = db.similarity_search(query, k=6) #k=6 just arbitary on basis or trial and error\n",
        "    doc_context = \"\\n\".join(d.page_content[:] for d in docs)\n",
        "\n",
        "    # 3) extract tickers/timeframe (best-effort)\n",
        "\n",
        "    market_summaries = []\n",
        "    for t in parsed_question[2]:\n",
        "        try:\n",
        "            tk = yf.Ticker(t)\n",
        "            if not parsed_question[3]:\n",
        "              hist = tk.history(period=\"1y\")\n",
        "              print(\"Feeding latest data\")\n",
        "            else:\n",
        "              parsed_question[3].sort()\n",
        "              hist = tk.history(start = parsed_question[3][0])\n",
        "              print(\"Feeding time specific dataa\")\n",
        "            if hist.empty:\n",
        "              continue\n",
        "\n",
        "            # compute compact statistics\n",
        "            first_close = hist[\"Close\"].iloc[0]\n",
        "            last_close  = hist[\"Close\"].iloc[-1]\n",
        "            pct_change  = (last_close / first_close - 1) * 100\n",
        "            mean_close   = hist[\"Close\"].mean()\n",
        "            volatility  = hist[\"Close\"].pct_change().std() * 100\n",
        "\n",
        "            market_summaries.append(\n",
        "                f\"{t}: last_close={last_close:.2f}, change_over_period={pct_change:.2f}%,\"\n",
        "                f\" mean={mean_close:.2f}, vol={volatility:.2f}% (period={len(hist)} days)\"\n",
        "            )\n",
        "        except Exception as e:\n",
        "            market_summaries.append(f\"{t}: failed to fetch ({e})\")\n",
        "\n",
        "    market_context = \"\\n\".join(market_summaries) or \"No market data available.\"\n",
        "\n",
        "    # 5) compose a concise instruction — keep it short to avoid token bloat\n",
        "    prompt = (\n",
        "        \"You are provided with TWO sources of context below. ANSWER the user's question \"\n",
        "        \"ONLY using the information in these contexts. FRAME an answer on the basis of whatever\"\n",
        "        \"document context is provided.YOU HAVE TO include the news provided in your answer\"\n",
        "        \"YOU HAVE TO give some reason for the market data on the basis of context you are provided\\n\\n\"\n",
        "        \"DOCUMENT CONTEXT:\\n\"\n",
        "        f\"{doc_context}\\n\\n\"\n",
        "        \"MARKET SUMMARY (compressed):\\n\"\n",
        "        f\"{market_context}\\n\\n\"\n",
        "        f\"USER QUESTION: {query}\\n\\n\"\n",
        "    )\n",
        "    # print(prompt)\n",
        "    return prompt\n"
      ],
      "metadata": {
        "id": "hUvPa1-i2Nir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.agents import create_agent\n",
        "agent = create_agent(model, tools=[], middleware=[inject_rag_and_market])#defining the middleware"
      ],
      "metadata": {
        "id": "tM6ZyfzjM9Mh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedder = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
      ],
      "metadata": {
        "id": "Y4_QNX1PgFx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "db = 0\n",
        "def build_database(question): #This is the function which brings everything together\n",
        "  parsed_question = parse_question(question)\n",
        "  add_tickers(parsed_question)\n",
        "  time.sleep(1.5) #Not to overuse the alphavantage api\n",
        "  docs = build_docs(parsed_question[2], parsed_question=parsed_question)\n",
        "\n",
        "  splitter = RecursiveCharacterTextSplitter(\n",
        "      chunk_size=1100,\n",
        "      chunk_overlap=100,\n",
        "  )\n",
        "\n",
        "  chunked_docs = splitter.split_documents(docs)\n",
        "  generate_stock_chart(parsed_question, parsed_question[2][0])\n",
        "  print(f\"Chunked into {len(chunked_docs)} passages\")\n",
        "  if chunked_docs:\n",
        "    global db\n",
        "    db = FAISS.from_documents(chunked_docs, embedder)\n",
        "    db.save_local(\"faiss_av_index\")\n",
        "  else:\n",
        "    print(\"No documents to chunk or embed. FAISS index will not be created.\")"
      ],
      "metadata": {
        "id": "bQD1BCXRNWmw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# question = 'Explain the trends is stocks of apple inc'\n",
        "# build_database(question)\n",
        "# query = question\n",
        "# for step in agent.stream(\n",
        "#     {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
        "#     stream_mode=\"values\",\n",
        "# ):\n",
        "#     step[\"messages\"][-1].pretty_print()"
      ],
      "metadata": {
        "id": "irBD_jsCNCZF",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_stock_chart(parsed_question, ticker):\n",
        "  tk = yf.Ticker(ticker=ticker)\n",
        "  if not parsed_question[3]:\n",
        "    history_df = tk.history(period=\"1mo\")\n",
        "    print(\"Feeding latest data\")\n",
        "  else:\n",
        "    parsed_question[3].sort()\n",
        "    history_df = tk.history(start = parsed_question[3][0])\n",
        "    print(\"Feeding time specific dataa\")\n",
        "  if history_df.empty:\n",
        "    return None\n",
        "  plt.figure(figsize=(10, 5))\n",
        "  plt.plot(history_df.index, history_df['Close'], label='Close Price', color='#2196F3', linewidth=2)\n",
        "\n",
        "  # Adding titles and labels\n",
        "  plt.title(f\"{ticker} Price Trend\", fontsize=14, fontweight='bold')\n",
        "  plt.xlabel(\"Date\", fontsize=10)\n",
        "  plt.ylabel(\"Price (INR)\", fontsize=10)\n",
        "  plt.grid(True, linestyle='--', alpha=0.7)\n",
        "  plt.legend()\n",
        "  plt.tight_layout()\n",
        "\n",
        "  # Save locally\n",
        "  chart_path = \"stock_chart.png\"\n",
        "  plt.savefig(chart_path)\n",
        "  plt.close() # Close to free up memory\n",
        "  return chart_path"
      ],
      "metadata": {
        "id": "hETbYdNHB8Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"Explain the AAPL stocks last month\"\n",
        "parsed_question = parse_question(question)\n",
        "generate_stock_chart(parsed_question, parsed_question[2][0])"
      ],
      "metadata": {
        "id": "oTJcj6UZFhSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "\n",
        "def chat_handler(question, history):\n",
        "  build_database(question)\n",
        "  query = question\n",
        "  partial_text = \"\"\n",
        "  for step in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
        "    stream_mode=\"values\",\n",
        "    ):\n",
        "      partial_text = step[\"messages\"][-1].content\n",
        "  #     yield partial_text + f\"\\n\\n![Current Trend](/image.jpg/)\"\n",
        "  # yield f\"\\n\\n### Market Trend Analysis\\n![Price Chart](file=/content/stock_chart.png)\"\n",
        "  img_path = \"/content/stock_chart.png\"\n",
        "  return {\n",
        "        \"text\": partial_text,\n",
        "        \"files\": [img_path]\n",
        "    }\n",
        "# Launch the interface\n",
        "demo = gr.ChatInterface(chat_handler)\n",
        "demo.queue().launch(\n",
        "    share = True,\n",
        "    show_error = True,\n",
        "    allowed_paths = [\".\"]\n",
        ")"
      ],
      "metadata": {
        "id": "JPNoCOFnYg0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jJeyI1ObZphj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}